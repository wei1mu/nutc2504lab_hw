import os
import base64
import requests
import operator
from typing import Annotated, List, TypedDict
from playwright.sync_api import sync_playwright
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage
from langgraph.graph import StateGraph, END

# --- 1. é…ç½®èˆ‡æ¨¡å‹åˆå§‹åŒ– ---
API_KEY = "ä½ çš„_API_KEY" # â¬…ï¸ é€™è£¡ä¸€å®šè¦æ”¹ï¼
SEARXNG_URL = "https://puli-8080.huannago.com/search"

# å®šç¾©æ¨¡å‹
vlm_llm = ChatOpenAI(base_url="https://ws-02.wade0426.me/v1", api_key=API_KEY, model="google/gemma-3-27b-it", temperature=0)
main_llm = ChatOpenAI(base_url="https://ws-03.wade0426.me/v1", api_key=API_KEY, model="/models/gpt-oss-120b", temperature=0)

# --- 2. ç‹€æ…‹å®šç¾© ---
class AgentState(TypedDict):
    input: str
    knowledge_base: Annotated[list, operator.add]
    queries: List[str]
    is_sufficient: bool
    cache_hit: bool

# --- 3. æ ¸å¿ƒå·¥å…·å‡½æ•¸ ---
def search_searxng(query: str):
    print(f"ğŸ” [å·¥å…·] æ­£åœ¨æœå°‹: {query}...")
    try:
        res = requests.get(SEARXNG_URL, params={"q": query, "format": "json"}, timeout=10).json()
        return res.get('results', [])[:1]
    except: return []

def vlm_read(url: str):
    print(f"ğŸ“¸ [è¦–è¦º] æ­£åœ¨è®€å–ç¶²é : {url}...")
    with sync_playwright() as p:
        try:
            browser = p.chromium.launch(headless=True)
            page = browser.new_page()
            page.goto(url, timeout=30000)
            img_b64 = base64.b64encode(page.screenshot()).decode('utf-8')
            browser.close()
            msg = HumanMessage(content=[
                {"type": "text", "text": "æ‘˜è¦æ­¤ç¶²é äº‹å¯¦"},
                {"type": "image_url", "image_url": {"url": f"data:image/png;base64,{img_b64}"}}
            ])
            return vlm_llm.invoke([msg]).content
        except Exception as e: return f"è®€å–å¤±æ•—: {e}"

# --- 4. LangGraph ç¯€é» ---
def check_cache_node(state: AgentState):
    print("â¡ï¸ [ç¯€é»] æª¢æŸ¥å¿«å–ä¸­...")
    cache = {"ä½ å¥½": "ä½ å¥½ï¼æˆ‘æ˜¯æŸ¥è­‰åŠ©æ‰‹ã€‚"} # ç°¡å–®å¿«å–ç¯„ä¾‹
    if state["input"] in cache:
        return {"knowledge_base": [cache[state["input"]]], "cache_hit": True}
    return {"cache_hit": False}

def planner_node(state: AgentState):
    print("â¡ï¸ [ç¯€é»] æ±ºç­–è©•ä¼°ä¸­...")
    if state.get("cache_hit") or len(state["knowledge_base"]) > 0:
        return {"is_sufficient": True}
    return {"is_sufficient": False}

def query_gen_node(state: AgentState):
    print("â¡ï¸ [ç¯€é»] ç”Ÿæˆé—œéµå­—...")
    query = main_llm.invoke(f"ç”Ÿæˆé—œéµå­—: {state['input']}").content
    return {"queries": [query]}

def search_tool_node(state: AgentState):
    print("â¡ï¸ [ç¯€é»] åŸ·è¡Œæª¢ç´¢èˆ‡è¦–è¦ºè™•ç†...")
    res = search_searxng(state["queries"][-1])
    info = vlm_read(res[0]['url']) if res else "æŸ¥ç„¡è³‡æ–™"
    return {"knowledge_base": [info]}

# --- 5. æ§‹å»ºæµç¨‹åœ– ---
workflow = StateGraph(AgentState)
workflow.add_node("check_cache", check_cache_node)
workflow.add_node("planner", planner_node)
workflow.add_node("query_gen", query_gen_node)
workflow.add_node("search_tool", search_tool_node)

workflow.set_entry_point("check_cache")
workflow.add_conditional_edges("check_cache", lambda x: "end" if x["cache_hit"] else "plan", {"end": END, "plan": "planner"})
workflow.add_conditional_edges("planner", lambda x: "y" if x["is_sufficient"] else "n", {"y": END, "n": "query_gen"})
workflow.add_edge("query_gen", "search_tool")
workflow.add_edge("search_tool", "planner")

app = workflow.compile()

# --- 6. åŸ·è¡Œæ¸¬è©¦ (é€™ä¸€æ®µä¿è­‰æœ‰è¼¸å‡ºï¼) ---
if __name__ == "__main__":
    test_input = input()
    print(f"\nğŸš€ å•Ÿå‹•ä»»å‹™: {test_input}")
    
    # ä½¿ç”¨ stream ç¢ºä¿æ¯å€‹æ­¥é©Ÿéƒ½å°å‡ºä¾†
    for output in app.stream({"input": test_input, "knowledge_base": [], "queries": []}):
        for node, data in output.items():
            print(f"âœ… {node} åŸ·è¡Œå®Œæˆï¼Œç›®å‰çŸ¥è­˜åº«ç­†æ•¸: {len(data.get('knowledge_base', []))}")
    
    print("\nâœ¨ ä»»å‹™åœ“æ»¿çµæŸï¼")